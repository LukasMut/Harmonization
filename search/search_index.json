{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Harmonizing the object recognition strategies of deep neural networks with humans Thomas Fel*, Ivan Felipe Rodriguez*, Drew Linsley*, Thomas Serre Carney Institute for Brain Science, Brown University, Providence, RI 02912 {thomas_fel,ivan_felipe_rodriguez,drew_linsley,thomas_serre}@brown.edu Read the official paper \u00bb Explore results \u00b7 Github \u00b7 Models zoo \u00b7 Tutorials \u00b7 Click-me paper Paper summary \u00b6 The many successes of deep neural networks (DNNs) over the past decade have largely been driven by computational scale rather than insights from biological intelligence. Here, we explore if these trends have also carried concomitant improvements in explaining visual strategies underlying human object recognition. We do this by comparing two related but distinct properties of visual strategies in humans and DNNs: where they believe important visual features are in images and how they use those features to categorize objects. Across 85 different DNNs and three independent datasets measuring human visual strategies on ImageNet, we find a trade-off between DNN top-1 categorization accuracy and their alignment with humans. State-of-the-art DNNs are progressively becoming less aligned with humans. We rectify this growing issue by introducing the harmonization procedure: a general-purpose training routine that aligns DNN and human visual strategies while improving object classification performance. Aligning the Gradients \u00b6 Human and DNNs rely on different features to recognize objects. In contrast, our neural harmonizer aligns DNN feature importance with humans. Gradients are smoothed from both humans and DNNs with a Gaussian kernel to improve visualization. Breaking the trade-off between performance and alignment \u00b6 The trade-off between DNN performance and alignment with human feature importance from the ClickMe dataset. Human feature alignment is the mean Spearman correlation between human and DNN feature importance maps, normalized by the average inter-rater alignment of humans. The grey-shaded region illustrates the convex hull of the trade-off between ImageNet accuracy and human feature alignment. All the models trained with the harmonization procedure are more accurate and aligned than versions of those models trained only for classification. Arrows denote a shift in performance after training with the harmonization procedure. Authors \u00b6 Thomas Fel* Ivan Felipe Rodriguez* Drew Linsley* Thomas Serre * : all authors have contributed equally. \ud83d\uddde\ufe0f Citation \u00b6 If you use or build on our work as part of your workflow in a scientific publication, please consider citing the official paper : @article{fel2022aligning, title={Harmonizing the object recognition strategies of deep neural networks with humans}, author={Fel, Thomas and Felipe, Ivan and Linsley, Drew and Serre, Thomas}, journal={Advances in Neural Information Processing Systems (NeurIPS)}, year={2022} } Moreover, this paper relies heavily on previous work from the Lab, notably Learning What and Where to Attend where the ambitious ClickMe dataset was collected. @article{linsley2018learning, title={Learning what and where to attend}, author={Linsley, Drew and Shiebler, Dan and Eberhardt, Sven and Serre, Thomas}, journal={International Conference on Learning Representations (ICLR)}, year={2019} } Tutorials \u00b6 Evaluate your own model (pytorch and tensorflow) \ud83d\udcdd License \u00b6 The package is released under MIT license .","title":"Home"},{"location":"#paper-summary","text":"The many successes of deep neural networks (DNNs) over the past decade have largely been driven by computational scale rather than insights from biological intelligence. Here, we explore if these trends have also carried concomitant improvements in explaining visual strategies underlying human object recognition. We do this by comparing two related but distinct properties of visual strategies in humans and DNNs: where they believe important visual features are in images and how they use those features to categorize objects. Across 85 different DNNs and three independent datasets measuring human visual strategies on ImageNet, we find a trade-off between DNN top-1 categorization accuracy and their alignment with humans. State-of-the-art DNNs are progressively becoming less aligned with humans. We rectify this growing issue by introducing the harmonization procedure: a general-purpose training routine that aligns DNN and human visual strategies while improving object classification performance.","title":"Paper summary"},{"location":"#authors","text":"Thomas Fel* Ivan Felipe Rodriguez* Drew Linsley* Thomas Serre * : all authors have contributed equally.","title":"Authors"},{"location":"#citation","text":"If you use or build on our work as part of your workflow in a scientific publication, please consider citing the official paper : @article{fel2022aligning, title={Harmonizing the object recognition strategies of deep neural networks with humans}, author={Fel, Thomas and Felipe, Ivan and Linsley, Drew and Serre, Thomas}, journal={Advances in Neural Information Processing Systems (NeurIPS)}, year={2022} } Moreover, this paper relies heavily on previous work from the Lab, notably Learning What and Where to Attend where the ambitious ClickMe dataset was collected. @article{linsley2018learning, title={Learning what and where to attend}, author={Linsley, Drew and Shiebler, Dan and Eberhardt, Sven and Serre, Thomas}, journal={International Conference on Learning Representations (ICLR)}, year={2019} }","title":"\ud83d\uddde\ufe0f Citation"},{"location":"#tutorials","text":"Evaluate your own model (pytorch and tensorflow)","title":"Tutorials"},{"location":"#license","text":"The package is released under MIT license .","title":"\ud83d\udcdd License"},{"location":"evaluation/","text":"In order to evaluate your own model on the benchmark, we have made available two notebooks showing how to do it from tensorflow or pytorch. Or, you can simply use the api directly as follows: from harmonization.common import load_clickme_val from harmonization.evaluation import evaluate_clickme clickme_dataset = load_clickme_val ( batch_size = 128 ) scores = evaluate_clickme ( model = model , # tensorflow or pytorch model clickme_val_dataset = clickme_dataset , preprocess_inputs = preprocessing_function ) print ( scores [ 'alignment_score' ]) Warning If you are using a Pytorch model, you need to specify a explainer function (see the pytorch notebook).","title":"Evaluation"},{"location":"models/","text":"Model zoo \u00b6 In our experiments, we re-trained a set of models with the harmonization loss proposed in the paper. You can easily download the weights of each models here: ViT B16 Harmonized : serrelab/prj_harmonization/vit_b16_harmonized VGG16 Harmonized : serrelab/prj_harmonization/vgg16_harmonized ResNet50V2 Harmonized : serrelab/prj_harmonization/resnet50v2_harmonized EfficientNet B0 : serrelab/prj_harmonization/efficientnet_b0 LeViT : serrelab/prj_harmonization/levit ConvNeXT : serrelab/prj_harmonization/convnext MaxViT : serrelab/prj_harmonization/maxvit In order to load them easily, we have set up utilities in the github repository. For example, to load the model lives harmonized: from harmonization.models import ( load_ViT_B16 , load_ResNet50 , load_VGG16 , load_EfficientNetB0 , load_tiny_ConvNeXT , load_tiny_MaxViT , load_LeViT_small , preprocess_input ) vit_harmonized = load_ViT_B16 () vgg_harmonized = load_VGG16 () resnet_harmonized = load_ResNet50 () efficient_harmonized = load_EfficientNetB0 () convnext_harmonized = load_tiny_ConvNeXT () maxvit_harmonized = load_tiny_MaxViT () levit_harmonized = load_LeViT_small () # load images (in [0, 255]) # ... images = preprocess_input ( images ) predictions = vit_harmonized ( images )","title":"Models"},{"location":"models/#model-zoo","text":"In our experiments, we re-trained a set of models with the harmonization loss proposed in the paper. You can easily download the weights of each models here: ViT B16 Harmonized : serrelab/prj_harmonization/vit_b16_harmonized VGG16 Harmonized : serrelab/prj_harmonization/vgg16_harmonized ResNet50V2 Harmonized : serrelab/prj_harmonization/resnet50v2_harmonized EfficientNet B0 : serrelab/prj_harmonization/efficientnet_b0 LeViT : serrelab/prj_harmonization/levit ConvNeXT : serrelab/prj_harmonization/convnext MaxViT : serrelab/prj_harmonization/maxvit In order to load them easily, we have set up utilities in the github repository. For example, to load the model lives harmonized: from harmonization.models import ( load_ViT_B16 , load_ResNet50 , load_VGG16 , load_EfficientNetB0 , load_tiny_ConvNeXT , load_tiny_MaxViT , load_LeViT_small , preprocess_input ) vit_harmonized = load_ViT_B16 () vgg_harmonized = load_VGG16 () resnet_harmonized = load_ResNet50 () efficient_harmonized = load_EfficientNetB0 () convnext_harmonized = load_tiny_ConvNeXT () maxvit_harmonized = load_tiny_MaxViT () levit_harmonized = load_LeViT_small () # load images (in [0, 255]) # ... images = preprocess_input ( images ) predictions = vit_harmonized ( images )","title":"Model zoo"},{"location":"results/","text":"Harmonized ViT vs ViT \u00b6 Click-Me (Human) Baseline model Harmonized model Harmonized VGG vs VGG \u00b6 Click-Me (Human) Baseline model Harmonized model Harmonized EfficientNetB0 vs EfficientNetB0 \u00b6 Click-Me (Human) Baseline model Harmonized model Harmonized ResNet50 vs ResNet50 \u00b6 Click-Me (Human) Baseline model Harmonized model window.addEventListener('DOMContentLoaded', function() { function is_ascendent(parent, child) { var node = child; while (node != null) { if (node == parent) { return true; } node = node.parentNode; } return false; } function event_prevent_default(e) { e = e || window.event; if (e.preventDefault) e.preventDefault(); e.returnValue = false; } const NB_IMAGES = 100 const SCROLL_POWER = 50 const GALLERY_DATA = [ ['vit', 'vit_baseline_faithful-wind.h5', 'vit_harmonized_solar-shadow.h5'], ['vgg', 'vgg16', 'vgg_frosty_eon'], ['effnet', 'efficientnet_b0', 'efficientnet_stellar-frog_8.h5'], ['resnet', 'resnet50_baseline', 'saliency_volcanic_monkey'], ]; GALLERY_DATA.forEach((data) => { [gallery_name, model_baseline, model_harmonized] = data const gallery = document.getElementById(gallery_name) const horizontal_scroll_event = (e) => { if (is_ascendent(gallery, e.target)) { if (e.deltaY > 0) gallery.scrollLeft += SCROLL_POWER; else gallery.scrollLeft -= SCROLL_POWER; window.scrollTop -= e.wheelDeltaY; e.preventDefault(); e.stopPropagation(); } } window.addEventListener(\"wheel\", horizontal_scroll_event, { passive: false }) const create_single_sample = (id) => { return ` <div class=\"single-sample\"> <img class=\"explanation\" loading=\"lazy\" src=\"https://storage.googleapis.com/serrelab/prj_harmonization/qualitative_data/clickme/${id}.png\"> <img class=\"explanation\" loading=\"lazy\" src=\"https://storage.googleapis.com/serrelab/prj_harmonization/qualitative_data/${model_baseline}/${id}.png\"> <img class=\"explanation harmonized\" loading=\"lazy\" src=\"https://storage.googleapis.com/serrelab/prj_harmonization/qualitative_data/${model_harmonized}/${id}.png\"> </div> ` } for (let i = 0; i < NB_IMAGES; i++) { gallery.innerHTML += create_single_sample(i) } }) }); .single-sample { display: inline-flex; flex-direction: column; } .gallery-container { position: relative; } .gallery { overflow-x: auto; overflow-y: hidden; white-space: nowrap; position: relative; padding: 20px; padding-left: 105px; box-sizing: border-box; } ::-webkit-scrollbar { display: none; } body { -ms-overflow-style: none; /* IE and Edge */ scrollbar-width: none; /* Firefox */ } .explanation { width: 150px; border: solid 0px; background: transparent; box-shadow: 0 1px 3px rgba(0,0,0,0.12), 0 1px 2px rgba(0,0,0,0.24); border: solid 3px transparent; margin: 2px; border-radius: 5px; } .explanation.harmonized { border-color: var(--primary); } .overlay-left { position: absolute; left: 0; top:0; background: var(--md-default-bg-color); height: 100%; display: flex; flex-direction: column; justify-content: space-around; text-align: center; width: 100px; flex-wrap: wrap; overflow-x: hidden; white-space: initial; z-index: 2; font-size: 18px }","title":"Results"},{"location":"results/#harmonized-vit-vs-vit","text":"Click-Me (Human) Baseline model Harmonized model","title":"Harmonized ViT vs ViT"},{"location":"results/#harmonized-vgg-vs-vgg","text":"Click-Me (Human) Baseline model Harmonized model","title":"Harmonized VGG vs VGG"},{"location":"results/#harmonized-efficientnetb0-vs-efficientnetb0","text":"Click-Me (Human) Baseline model Harmonized model","title":"Harmonized EfficientNetB0 vs EfficientNetB0"},{"location":"results/#harmonized-resnet50-vs-resnet50","text":"Click-Me (Human) Baseline model Harmonized model window.addEventListener('DOMContentLoaded', function() { function is_ascendent(parent, child) { var node = child; while (node != null) { if (node == parent) { return true; } node = node.parentNode; } return false; } function event_prevent_default(e) { e = e || window.event; if (e.preventDefault) e.preventDefault(); e.returnValue = false; } const NB_IMAGES = 100 const SCROLL_POWER = 50 const GALLERY_DATA = [ ['vit', 'vit_baseline_faithful-wind.h5', 'vit_harmonized_solar-shadow.h5'], ['vgg', 'vgg16', 'vgg_frosty_eon'], ['effnet', 'efficientnet_b0', 'efficientnet_stellar-frog_8.h5'], ['resnet', 'resnet50_baseline', 'saliency_volcanic_monkey'], ]; GALLERY_DATA.forEach((data) => { [gallery_name, model_baseline, model_harmonized] = data const gallery = document.getElementById(gallery_name) const horizontal_scroll_event = (e) => { if (is_ascendent(gallery, e.target)) { if (e.deltaY > 0) gallery.scrollLeft += SCROLL_POWER; else gallery.scrollLeft -= SCROLL_POWER; window.scrollTop -= e.wheelDeltaY; e.preventDefault(); e.stopPropagation(); } } window.addEventListener(\"wheel\", horizontal_scroll_event, { passive: false }) const create_single_sample = (id) => { return ` <div class=\"single-sample\"> <img class=\"explanation\" loading=\"lazy\" src=\"https://storage.googleapis.com/serrelab/prj_harmonization/qualitative_data/clickme/${id}.png\"> <img class=\"explanation\" loading=\"lazy\" src=\"https://storage.googleapis.com/serrelab/prj_harmonization/qualitative_data/${model_baseline}/${id}.png\"> <img class=\"explanation harmonized\" loading=\"lazy\" src=\"https://storage.googleapis.com/serrelab/prj_harmonization/qualitative_data/${model_harmonized}/${id}.png\"> </div> ` } for (let i = 0; i < NB_IMAGES; i++) { gallery.innerHTML += create_single_sample(i) } }) }); .single-sample { display: inline-flex; flex-direction: column; } .gallery-container { position: relative; } .gallery { overflow-x: auto; overflow-y: hidden; white-space: nowrap; position: relative; padding: 20px; padding-left: 105px; box-sizing: border-box; } ::-webkit-scrollbar { display: none; } body { -ms-overflow-style: none; /* IE and Edge */ scrollbar-width: none; /* Firefox */ } .explanation { width: 150px; border: solid 0px; background: transparent; box-shadow: 0 1px 3px rgba(0,0,0,0.12), 0 1px 2px rgba(0,0,0,0.24); border: solid 3px transparent; margin: 2px; border-radius: 5px; } .explanation.harmonized { border-color: var(--primary); } .overlay-left { position: absolute; left: 0; top:0; background: var(--md-default-bg-color); height: 100%; display: flex; flex-direction: column; justify-content: space-around; text-align: center; width: 100px; flex-wrap: wrap; overflow-x: hidden; white-space: initial; z-index: 2; font-size: 18px }","title":"Harmonized ResNet50 vs ResNet50"},{"location":"training/","text":"In order to train your own harmonized model, we have made available a way to simply load the ClickMe training set, as well as the harmonization loss we have used in the paper. Loading ClickMe training set \u00b6 First, you will need to load the training dataset: from harmonization.common import load_clickme_train clickme_ds = load_clickme_train ( batch_size = 128 ) for images , heatmaps , labels in clickme_ds : print ( images . shape ) # (128, 224, 224, 3) print ( heatmaps . shape ) # (128, 224, 224, 1) print ( labels . shape ) # (128, 1000) Note that, if you already have the shards locally, you can also load the dataset using the load_clickme function: from harmonization.common import load_clickme clickme_ds = load_clickme_train ( shards_paths = [ 'dataset/train_clickme_0' , 'dataset/train_clickme_1' ... ], batch_size = 128 ) Using the Harmonization loss \u00b6 Now that we know how to load the training set, we just need the harmonization loss: def harmonizer_loss ( model , images , tokens , labels , true_heatmaps , cross_entropy = tf . keras . losses . CategoricalCrossentropy (), lambda_weights = 1e-5 , lambda_harmonization = 1.0 ): ... To use the loss, simply call the function with your model, the images / labels and heatmaps for ClickMe: from harmonization.training import harmonizer_loss ... # loading dataset for images , heatmaps , labels in clickme_ds : tokens = tf . ones ( len ( images )) # tokens are flags to indicate if the image have an associated heatmap loss = harmonizer_loss ( model , images , tokens , labels , heatmaps ) For example, if we decide to mix the ClickMe dataset with ImageNet, we may not have heatmaps for each images, in that case we can use the tokens flag parameters to designate when an heatmaps is provided ( 1 means heatmaps provided).","title":"Training"},{"location":"training/#loading-clickme-training-set","text":"First, you will need to load the training dataset: from harmonization.common import load_clickme_train clickme_ds = load_clickme_train ( batch_size = 128 ) for images , heatmaps , labels in clickme_ds : print ( images . shape ) # (128, 224, 224, 3) print ( heatmaps . shape ) # (128, 224, 224, 1) print ( labels . shape ) # (128, 1000) Note that, if you already have the shards locally, you can also load the dataset using the load_clickme function: from harmonization.common import load_clickme clickme_ds = load_clickme_train ( shards_paths = [ 'dataset/train_clickme_0' , 'dataset/train_clickme_1' ... ], batch_size = 128 )","title":"Loading ClickMe training set"},{"location":"training/#using-the-harmonization-loss","text":"Now that we know how to load the training set, we just need the harmonization loss: def harmonizer_loss ( model , images , tokens , labels , true_heatmaps , cross_entropy = tf . keras . losses . CategoricalCrossentropy (), lambda_weights = 1e-5 , lambda_harmonization = 1.0 ): ... To use the loss, simply call the function with your model, the images / labels and heatmaps for ClickMe: from harmonization.training import harmonizer_loss ... # loading dataset for images , heatmaps , labels in clickme_ds : tokens = tf . ones ( len ( images )) # tokens are flags to indicate if the image have an associated heatmap loss = harmonizer_loss ( model , images , tokens , labels , heatmaps ) For example, if we decide to mix the ClickMe dataset with ImageNet, we may not have heatmaps for each images, in that case we can use the tokens flag parameters to designate when an heatmaps is provided ( 1 means heatmaps provided).","title":"Using the Harmonization loss"}]}